{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd166112",
   "metadata": {},
   "source": [
    "### 4. 함수형 API를 사용해 복잡한 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a85e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1326d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation= \"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6771e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30)           930         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7141ec",
   "metadata": {},
   "source": [
    "- 일부 특성은 짧은 경로로 전달하고 다른 특성들은 깊은 경로로 전달하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44fd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5],name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6],name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation =\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation =\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A,hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A,input_B],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feef7964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 981us/step - loss: 1.7854 - val_loss: 0.8606\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 859us/step - loss: 0.8098 - val_loss: 0.7289\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 955us/step - loss: 0.7085 - val_loss: 0.6698\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.6528 - val_loss: 0.6313\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.6147 - val_loss: 0.6024\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 943us/step - loss: 0.5859 - val_loss: 0.5804\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 850us/step - loss: 0.5636 - val_loss: 0.5628\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.5460 - val_loss: 0.5489\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.5326 - val_loss: 0.5378\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.5218 - val_loss: 0.5295\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 632us/step - loss: 0.5128 - val_loss: 0.5225\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.5062 - val_loss: 0.5172\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.5005 - val_loss: 0.5132\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.4959 - val_loss: 0.5090\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.4922 - val_loss: 0.5056\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 668us/step - loss: 0.4890 - val_loss: 0.5034\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.4859 - val_loss: 0.5005\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.4834 - val_loss: 0.4984\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4807 - val_loss: 0.4967\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 599us/step - loss: 0.4786 - val_loss: 0.4945\n",
      "162/162 [==============================] - 0s 531us/step - loss: 0.4798\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e47e3",
   "metadata": {},
   "source": [
    "- 규제를 위한 보조 출력 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1539a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab896ec",
   "metadata": {},
   "source": [
    "각 출력은 자신만의 손실 함수가 필요함. 따라서 모델을 컴파일할 때 손실의 리스트를 전달해야 한다.(하나의 손실을 전달하면 케라스는  \n",
    "    모든 출력의 손실함수가 동일하다고 가정한다.)  \n",
    "기본적으로 케라스는 나열된 손실을 모두 더하여 최종 손실을 구해 훈련에 사용한다.  \n",
    "보조 출력보다 주 출력에 더 관심이 많다면(보조 출력은 규제로만 사용되므로), 주 출력의 손실에 더 많은 가중치를 부여해야 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e077afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\",\"mse\"],loss_weights=[0.9,0.1],optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aba380",
   "metadata": {},
   "source": [
    "모델을 훈련할 때 각 출력에 대한 레이블을 제공해야 한다. 여기에서는 주 출력과 보조출력이 같은 것을 예측해야 하므로 동일한  \n",
    "레이블을 사용한다. 따라서 y_train 대신에 (y_train,y_train)을 전달한다(y_valid와 y_test도 동일하다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b991fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0127 - main_output_loss: 0.9249 - aux_output_loss: 1.8033 - val_loss: 0.6360 - val_main_output_loss: 0.5817 - val_aux_output_loss: 1.1247\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 796us/step - loss: 0.6025 - main_output_loss: 0.5487 - aux_output_loss: 1.0868 - val_loss: 0.6793 - val_main_output_loss: 0.6494 - val_aux_output_loss: 0.9485\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 830us/step - loss: 0.5563 - main_output_loss: 0.5157 - aux_output_loss: 0.9217 - val_loss: 0.5322 - val_main_output_loss: 0.4983 - val_aux_output_loss: 0.8375\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.5105 - main_output_loss: 0.4754 - aux_output_loss: 0.8268 - val_loss: 0.5097 - val_main_output_loss: 0.4816 - val_aux_output_loss: 0.7624\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.5316 - main_output_loss: 0.5048 - aux_output_loss: 0.7733 - val_loss: 0.5038 - val_main_output_loss: 0.4802 - val_aux_output_loss: 0.7164\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.4830 - main_output_loss: 0.4565 - aux_output_loss: 0.7212 - val_loss: 0.5029 - val_main_output_loss: 0.4822 - val_aux_output_loss: 0.6893\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.4762 - main_output_loss: 0.4526 - aux_output_loss: 0.6887 - val_loss: 0.4815 - val_main_output_loss: 0.4609 - val_aux_output_loss: 0.6670\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.4566 - main_output_loss: 0.4338 - aux_output_loss: 0.6625 - val_loss: 0.4697 - val_main_output_loss: 0.4515 - val_aux_output_loss: 0.6333\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.4464 - main_output_loss: 0.4248 - aux_output_loss: 0.6409 - val_loss: 0.4599 - val_main_output_loss: 0.4419 - val_aux_output_loss: 0.6218\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.4909 - main_output_loss: 0.4755 - aux_output_loss: 0.6302 - val_loss: 0.4591 - val_main_output_loss: 0.4418 - val_aux_output_loss: 0.6144\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.4424 - main_output_loss: 0.4243 - aux_output_loss: 0.6055 - val_loss: 0.4424 - val_main_output_loss: 0.4260 - val_aux_output_loss: 0.5902\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 765us/step - loss: 0.4240 - main_output_loss: 0.4060 - aux_output_loss: 0.5859 - val_loss: 0.4335 - val_main_output_loss: 0.4181 - val_aux_output_loss: 0.5716\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 766us/step - loss: 0.4160 - main_output_loss: 0.3988 - aux_output_loss: 0.5706 - val_loss: 0.4280 - val_main_output_loss: 0.4134 - val_aux_output_loss: 0.5594\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 783us/step - loss: 0.4066 - main_output_loss: 0.3897 - aux_output_loss: 0.5586 - val_loss: 0.4189 - val_main_output_loss: 0.4046 - val_aux_output_loss: 0.5473\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.3988 - main_output_loss: 0.3825 - aux_output_loss: 0.5450 - val_loss: 0.4104 - val_main_output_loss: 0.3965 - val_aux_output_loss: 0.5359\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 802us/step - loss: 0.3945 - main_output_loss: 0.3786 - aux_output_loss: 0.5369 - val_loss: 0.4091 - val_main_output_loss: 0.3956 - val_aux_output_loss: 0.5303\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.3913 - main_output_loss: 0.3767 - aux_output_loss: 0.5230 - val_loss: 0.4176 - val_main_output_loss: 0.4048 - val_aux_output_loss: 0.5327\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3829 - main_output_loss: 0.3679 - aux_output_loss: 0.5184 - val_loss: 0.3965 - val_main_output_loss: 0.3836 - val_aux_output_loss: 0.5127\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.3786 - main_output_loss: 0.3641 - aux_output_loss: 0.5091 - val_loss: 0.4025 - val_main_output_loss: 0.3901 - val_aux_output_loss: 0.5135\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 798us/step - loss: 0.3736 - main_output_loss: 0.3594 - aux_output_loss: 0.5012 - val_loss: 0.3976 - val_main_output_loss: 0.3851 - val_aux_output_loss: 0.5100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A,X_train_B], [y_train,y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A,X_valid_B],[y_valid,y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f2a29",
   "metadata": {},
   "source": [
    "모델을 평가하면 케라스는 개별 손실과 함께 총 손실을 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7fe01a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 546us/step - loss: 0.3829 - main_output_loss: 0.3705 - aux_output_loss: 0.4950\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a966aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f7fcb7",
   "metadata": {},
   "source": [
    "여기에서 보는 것처럼 함수형 API는 원하는 어떤 종류의 구조도 손쉽게 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6e13d",
   "metadata": {},
   "source": [
    " ### 5.서브클래싱 API로 동적 모델 만들기\n",
    " 시퀸셜 API와 함수형 API는 모두 선언적이다. 사용할 층과 연결 방식을 먼저 정의해야 한다. 그 다음 모델에 데이터를 주입하여 훈련이나 추론을 시작할 수 있다. \n",
    " 하지만, 어떤 모델은 반복문을 포함하고 다양한 크기를 다루어야 하며 조건문을 가지는 등 여러 가지 동적인 구조를 필요로 한다.\n",
    " 이런 경우에 조금 더 명령형 프로그래밍 스타일이 필요하다면 서브클래싱API가 정답이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f94d51",
   "metadata": {},
   "source": [
    "- 서브 클래싱 api  \n",
    "    간단히 Model 클래스를 상속한 다음 생성자 안에서 필요한 층을 만든다. 그 다음 call() 메서드 안에 수행하려는 연산을 기술한다.  \n",
    "    예를 들어 다음 WideAndDeepModel 클래스의 인스턴스는 앞서 함수형 API로 만든 모델과 동일한 기능을 수행한다.  \n",
    "    이전에 했던 것처럼 이 인스턴스를 사용해 모델 컴파일,훈련,평가,예측을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8702c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # 표준 매개변수를 처리한다(예를 들면, name)\n",
    "        self.hidden1= keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2= keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1= self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0a337",
   "metadata": {},
   "source": [
    "### 6. 모델 저장과 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7f65d",
   "metadata": {},
   "source": [
    "시퀸셜 api와 함수형 api를 사용하면 훈련된 케라스 모델을 저장하는 것은 다음처럼 매우 쉽다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bef1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd6b655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 826us/step - loss: 2.0527 - val_loss: 0.7422\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.6884 - val_loss: 0.6451\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 594us/step - loss: 0.6278 - val_loss: 0.6178\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.5991 - val_loss: 0.5964\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.5779 - val_loss: 0.5806\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 526us/step - loss: 0.5605 - val_loss: 0.5656\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 642us/step - loss: 0.5458 - val_loss: 0.5529\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.5332 - val_loss: 0.5415\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.5216 - val_loss: 0.5311\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.5116 - val_loss: 0.5229\n",
      "162/162 [==============================] - 0s 461us/step - loss: 0.5201\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9d45ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a0464",
   "metadata": {},
   "source": [
    "훈련이 몇 시간 동안 지속되는 경우 훈련 마지막에 모델을 저장하는 것 뿐만이 아니라 훈련 도중 일정 간격으로 체크포인트를 저장해야 한다.  \n",
    "fit() 메서드에서 체크포인트를 저장하는 방법은 콜백을 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb3925cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52189dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5764959],\n",
       "       [1.2645893],\n",
       "       [3.3101127]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be2178",
   "metadata": {},
   "source": [
    "### 7. 콜백 사용하기\n",
    "fit()메서드의 callbacks 매개변수를 사용하여 케라스가 훈련의 시작이나 끝에 호출할 객체 리스트를 지정할 수 있다. 또는 에포크의  \n",
    "시작이나 끝, 각 배치 처리 전후에 호출할 수도 있다. 또는 에포크의 시작이나 끝, 각 배치 처리 전후에 호출할 수도 있다.  \n",
    "예를 들어 ModelCheckpoint는 훈련하는 동안 일정한 간격으로 모델의 체크포인트를 저장한다. 기본적으로 매 에포크의 끝에서 호출된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "154cac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43837dc",
   "metadata": {},
   "source": [
    "훈련하는 동안 검증 세트를 사용하면 ModelChecpoint를 만들 때 save_best_only=True로 지정할 수 있다.  \n",
    "이렇게 하면 최상의 검증 세트 점수에서만 모델을 저장한다. 오랜 훈련시간으로 훈련세트에 과대적합될 걱정을 하지 않아도 된다.  \n",
    "훈련이 끝난 후 마지막에 저장된 모델을 복원하면 된다. 그 모델이 검증 세트에서 최상의 점수를 낸 모델이다.  \n",
    "다음 코드는 조기 종료를 구현하는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb60459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.4630 - val_loss: 0.4270\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.4499 - val_loss: 0.4147\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 533us/step - loss: 0.4391 - val_loss: 0.4084\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.4306 - val_loss: 0.4033\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4237 - val_loss: 0.4014\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 629us/step - loss: 0.4174 - val_loss: 0.4271\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 664us/step - loss: 0.4125 - val_loss: 0.3956\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 611us/step - loss: 0.4081 - val_loss: 0.3834\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4041 - val_loss: 0.4004\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 521us/step - loss: 0.4008 - val_loss: 0.4120\n",
      "162/162 [==============================] - 0s 375us/step - loss: 0.4028\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # 최상의 모델로 롤백\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ab4bc",
   "metadata": {},
   "source": [
    "조기 종료를 구현하는 또 다른 방법은 EarlyStopping 콜백을 사용하는 것입니다. 일정 에포크(patience 매개변수로 지정한다)동안 검증 세트에 대한 점수가 향상되지 않으면 훈련을 멈춘다. 선택적으로 최상의 모델을 복원할 수도 있다. \n",
    "컴퓨터가 문제를 일으키는 경우를 대비해서 체크포인트 저장 콜백과 시간과 컴퓨팅 자원을 낭비하지 않기 위해 진전이 없는 경우 훈련을  \n",
    "일찍 멈추는 콜백을 함께 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de11207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a940414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4039 - val_loss: 0.4103\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 532us/step - loss: 0.4008 - val_loss: 0.3884\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.3979 - val_loss: 0.3738\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 608us/step - loss: 0.3953 - val_loss: 0.3858\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 597us/step - loss: 0.3926 - val_loss: 0.4135\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.3907 - val_loss: 0.3937\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.3884 - val_loss: 0.3910\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.3864 - val_loss: 0.4155\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.3850 - val_loss: 0.3698\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.3830 - val_loss: 0.3700\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.3812 - val_loss: 0.3611\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3798 - val_loss: 0.3644\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.3782 - val_loss: 0.4030\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.3772 - val_loss: 0.3588\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.3753 - val_loss: 0.4054\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.3743 - val_loss: 0.3596\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3731 - val_loss: 0.3554\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 612us/step - loss: 0.3715 - val_loss: 0.4507\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.3712 - val_loss: 0.3813\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.3699 - val_loss: 0.3543\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 610us/step - loss: 0.3687 - val_loss: 0.3664\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3679 - val_loss: 0.3502\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3670 - val_loss: 0.3763\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3659 - val_loss: 0.3697\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3651 - val_loss: 0.3490\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.3645 - val_loss: 0.3574\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.3635 - val_loss: 0.3560\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.3624 - val_loss: 0.3939\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3621 - val_loss: 0.3838\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 594us/step - loss: 0.3616 - val_loss: 0.3497\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.3605 - val_loss: 0.3963\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.3599 - val_loss: 0.3856\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.3594 - val_loss: 0.3430\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3585 - val_loss: 0.3744\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.3579 - val_loss: 0.3741\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.3574 - val_loss: 0.3600\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.3565 - val_loss: 0.3766\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3560 - val_loss: 0.3626\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.3557 - val_loss: 0.3468\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.3548 - val_loss: 0.3417\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3542 - val_loss: 0.3824\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.3538 - val_loss: 0.3415\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.3530 - val_loss: 0.3406\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 635us/step - loss: 0.3529 - val_loss: 0.3406\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.3524 - val_loss: 0.3435\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.3514 - val_loss: 0.4065\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.3516 - val_loss: 0.3367\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.3506 - val_loss: 0.4043\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3506 - val_loss: 0.3369\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.3493 - val_loss: 0.3898\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 559us/step - loss: 0.3495 - val_loss: 0.3505\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3489 - val_loss: 0.3447\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.3482 - val_loss: 0.3723\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3477 - val_loss: 0.3757\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.3476 - val_loss: 0.3358\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.3470 - val_loss: 0.3459\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.3467 - val_loss: 0.3316\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3460 - val_loss: 0.3830\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3456 - val_loss: 0.3855\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3453 - val_loss: 0.3531\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 532us/step - loss: 0.3449 - val_loss: 0.3525\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.3444 - val_loss: 0.3313\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 618us/step - loss: 0.3440 - val_loss: 0.4077\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3439 - val_loss: 0.3352\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 529us/step - loss: 0.3434 - val_loss: 0.3690\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 538us/step - loss: 0.3429 - val_loss: 0.3441\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.3426 - val_loss: 0.3404\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.3422 - val_loss: 0.3324\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.3419 - val_loss: 0.3355\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3412 - val_loss: 0.3565\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 537us/step - loss: 0.3410 - val_loss: 0.3360\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.3402 - val_loss: 0.3922\n",
      "162/162 [==============================] - 0s 472us/step - loss: 0.3482\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90d74d",
   "metadata": {},
   "source": [
    "- 사용자 정의 콜백 만들기  \n",
    "    다음의 콜백은 훈련하는 동안 검증 손실과 훈련손실의 비율을 출력한다.(즉 과대적합을 감지한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97434bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b635035f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 682us/step - loss: 0.3437 - val_loss: 0.3918\n",
      "\n",
      "val/train: 1.14\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87049123",
   "metadata": {},
   "source": [
    "### 8. 텐서보드를 사용해 시각화하기\n",
    "    텐서보드는 훈련 동안 학습 곡선을 그리거나 여러 실행 간의 학습 곡선을 비교하고 계싼 그래프 시각화와 훈련 통계 분석을 수행할 수 있다.  \n",
    "    또한 모델이 생성한 이미지를 확인하거나 3D에 투영된 복잡한 다차원 데이터를 시각화하고 자동으로 클러스터링을 해주는 등 많은 기능을 제공한다. 텐서보드는 텐서플로를 설치할 때 자동으로 설치되므로 이미 시스템에 준비되어 있다.\n",
    "    텐서보드를 사용하려면 프로그램을 수저앟여 이벤트파일event file 이라는 특별한 이진 로그 파일에 시각화 하려는 데이터를 출력해야 한다.  \n",
    "    각각의 이진 데이터 레코드를 서미리라고 부른다. \n",
    "    텐서보드 서버는 로그 디렉터리를 모니터링하고 자동으로 변경사항을 읽어 그래프를 업데이터 한다. 훈련하는 중가넹 학습 곡선 같이 실시간 데이터를 시각화 할 수 있다.  \n",
    "    일반적으로 텐서보드 서버가 루트root 로그 데렉토리를 가리키고 프로글매은 실행할 때마다 다른 서브디렉터리에 이벤트를 기록한다. \n",
    "    이렇게 하면 복잡하지 않게 하나의 텐서보드 서버가 여러 번 실행한 프로그램의 결과를 시각화하고 비교할 수 있다.\n",
    "    먼저 텐서보드 로그를 위해 사용할 루트 로그 디렉터리를 정의하겟다. 현재 날짜와 시간을 사용해 실행할 때마다 다른 서브디렉터리 경로를 생성하는 간단한 함수도 만들겠다. 테스트하는 하이퍼라미터 갑소가 같은 추가적인 정보를 로그 디렉터리 이름으로 사용할 수 있다.  \n",
    "    이렇게 하면 텐서보드에서 어던 로그인지 구분하기 편하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72da2463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "093dfd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2021_09_14-17_22_01'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfa5266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24b6dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 999us/step - loss: 2.4710 - val_loss: 2.2079\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.8573 - val_loss: 0.8581\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.6642 - val_loss: 0.6215\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.6112 - val_loss: 0.5645\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 0s 752us/step - loss: 0.5759 - val_loss: 0.5337\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.5465 - val_loss: 0.5055\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 0s 637us/step - loss: 0.5216 - val_loss: 0.4860\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.5005 - val_loss: 0.4638\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4831 - val_loss: 0.4476\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 828us/step - loss: 0.4682 - val_loss: 0.4397\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 0s 613us/step - loss: 0.4565 - val_loss: 0.4250\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4466 - val_loss: 0.4153\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 0s 649us/step - loss: 0.4382 - val_loss: 0.4086\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 945us/step - loss: 0.4314 - val_loss: 0.4072\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4257 - val_loss: 0.4072\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 0s 911us/step - loss: 0.4205 - val_loss: 0.4099\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.4163 - val_loss: 0.4117\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.4127 - val_loss: 0.4048\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.4092 - val_loss: 0.4046\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4062 - val_loss: 0.4023\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 832us/step - loss: 0.4033 - val_loss: 0.3957\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.4004 - val_loss: 0.3917\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3980 - val_loss: 0.4032\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.3956 - val_loss: 0.4038\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.3935 - val_loss: 0.3987\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.3912 - val_loss: 0.3844\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.3892 - val_loss: 0.3884\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 518us/step - loss: 0.3873 - val_loss: 0.4019\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 0s 530us/step - loss: 0.3853 - val_loss: 0.3971\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 700us/step - loss: 0.3834 - val_loss: 0.3968\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
